{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67FbkjESXdY7"
      },
      "source": [
        "# Assignment 1: Classifying handwritten digits using probability theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBLY9StIXdY-"
      },
      "source": [
        "# 1. Introduce the problem in your own words. You should mention what data we use, what we want to do with it and how we will do it. Explain with your own words the Naive Bayes classifier, it's assumptions and how this can classify the digits $0-9$.\n",
        "\n",
        "Write your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFMPTe6vXdY_"
      },
      "source": [
        "The data set cointains images with a black background and drawings of numbers from zero to nine, in white.  The objective is to build a classifier that can correctly distinguish what is the number drawn in the image.  For this, the Naive Bayes classifier will be used. This is an aproximation where each pixel is analysed indpendently of the others.  The advantage of this, is that the number of calclations required is significantly smaller, rather than analysing whole groups of pixels within the image.  To use the Naive Bayes classifier, every pixel is assumed to be independent of each other.  In practice, this may not be the case, but by using the Naive Bayes we only care about a broad degree of accuracy.  \n",
        "\n",
        "By combining Naive Bayes with a classification rule, we can attempt to implement this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HVn50R6zXdZA"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pylab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\nicho\\Documents\\General\\Academia\\DTU\\02462\\ass1_part1+3.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nicho/Documents/General/Academia/DTU/02462/ass1_part1%2B3.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m division\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nicho/Documents/General/Academia/DTU/02462/ass1_part1%2B3.ipynb#ch0000003?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nicho/Documents/General/Academia/DTU/02462/ass1_part1%2B3.ipynb#ch0000003?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpylab\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nicho/Documents/General/Academia/DTU/02462/ass1_part1%2B3.ipynb#ch0000003?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmpl_toolkits\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maxes_grid1\u001b[39;00m \u001b[39mimport\u001b[39;00m make_axes_locatable\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nicho/Documents/General/Academia/DTU/02462/ass1_part1%2B3.ipynb#ch0000003?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_confusion_matrix\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pylab'"
          ]
        }
      ],
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from sklearn.metrics import plot_confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kBxKnPpXdZB"
      },
      "source": [
        "# 2. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "I_gHIqM7XdZB",
        "outputId": "0a60dc72-0b04-43df-aaaa-2f672e0311fc"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\nicho\\Documents\\General\\Academia\\DTU\\02462\\ass1_part1+3.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nicho/Documents/General/Academia/DTU/02462/ass1_part1%2B3.ipynb#ch0000005?line=0'>1</a>\u001b[0m \u001b[39m# load data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nicho/Documents/General/Academia/DTU/02462/ass1_part1%2B3.ipynb#ch0000005?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mmnist_bin.npz\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, allow_pickle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m# insert your answer here\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nicho/Documents/General/Academia/DTU/02462/ass1_part1%2B3.ipynb#ch0000005?line=3'>4</a>\u001b[0m \u001b[39m# get vector representation of binary digits\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nicho/Documents/General/Academia/DTU/02462/ass1_part1%2B3.ipynb#ch0000005?line=4'>5</a>\u001b[0m X \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "data = np.load(\"mnist_bin.npz\") # insert your answer here\n",
        "## If running into further errors, consult \"np.load(\"mnist_bin.npz\",'',allow_pickle=False)\"\n",
        "\n",
        "# get vector representation of binary digits\n",
        "X = data['X']\n",
        "\n",
        "# get binary labels\n",
        "y = data['y']\n",
        "\n",
        "print('The shape of X is (%d, %d)' % X.shape)\n",
        "print('The shape of y is (%d)\\n' % y.shape)\n",
        "\n",
        "# Dimensions\n",
        "N, D = X.shape\n",
        "\n",
        "print('Number of images: %d' % N)\n",
        "print('Number of pixels: %d' % D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX9HHhVYXdZC"
      },
      "source": [
        "#### Run the code beneath. It plots 10 images of each digit. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU9RXIznXdZC"
      },
      "outputs": [],
      "source": [
        "# Graphical representation function, don't worry about it\n",
        "def show_image(x, title=\"\", clim=None, cmap=plt.cm.gray, colorbar=False):\n",
        "    ax = plt.gca()\n",
        "    im = ax.imshow(x.reshape((28, 28)), cmap=cmap, clim=clim)\n",
        "    \n",
        "    if len(title) > 0:\n",
        "        plt.title(title)\n",
        "        \n",
        "    plt.axis('off')\n",
        "    \n",
        "    if colorbar:\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "        plt.colorbar(im, cax=cax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvFkLmPFXdZD"
      },
      "outputs": [],
      "source": [
        "# More graphs\n",
        "\n",
        "num_images_per_row  = 10\n",
        "num_labels          = 10\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "counter             = 1\n",
        "for i in range(num_labels):\n",
        "    for l in range(num_images_per_row):\n",
        "        plt.subplot(num_labels, num_images_per_row, counter)\n",
        "\n",
        "        all_images_belonging_to_class_l = X[y==l,:]\n",
        "        one_images_belonging_to_class_l = all_images_belonging_to_class_l[i]\n",
        "        \n",
        "        show_image(one_images_belonging_to_class_l)  \n",
        "        counter += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qckrWIHnXdZE"
      },
      "source": [
        "\n",
        "# 3. Explain what a training set is, what a test set is and why we as data scientists *always* split data into test/train before doing any modelling. What do we want to avoid?\n",
        "\n",
        "A training set is a set of data that closely approximates the actual conditions of the sample you wish to study, without infringing on the specific conditions of the elements which you wish to test.  While we could both train and test on the same large set, the results would be suspiciously accurate, as the model would know the precise conditions for the testing set having encountered it in training, and the result would be falsely indicative of the true ability of the classifier.\n",
        "\n",
        "Therefore, data scientists always split data into test/train and take great care in this, to avoid invalidating their own results with suspiciously high accuracies by leaks between the sets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLv-8d6OXdZE"
      },
      "outputs": [],
      "source": [
        "# No reason to change this, particularly.  It's a sensible split between test and train,\n",
        "# and trying to prune the priors won't solve our problem\n",
        "\n",
        "N = len(X)\n",
        "N_train = int(0.8*N)\n",
        "N_test = N-N_train\n",
        "\n",
        "# set random seed:\n",
        "np.random.seed(0) # don't change this :-)\n",
        "\n",
        "# create a random permutation for splitting into training and test\n",
        "randperm = np.random.permutation(N)\n",
        "\n",
        "# split into training and test\n",
        "train_idx = randperm[:N_train]\n",
        "test_idx = randperm[N_train:]\n",
        "Xtrain, Xtest = X[train_idx, :], X[test_idx, :]\n",
        "ytrain, ytest = y[train_idx], y[test_idx]\n",
        "\n",
        "print('Total number of images:\\t\\t%d' % N)\n",
        "print('Number of training images:\\t%d' % N_train)\n",
        "print('Number of test images:\\t\\t%d' % N_test)\n",
        "print(ytrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyeRqEReXdZF"
      },
      "source": [
        "# 4. Implement/change the code to handle all digits. \n",
        "### Fitting the Naı̈ve-Bayes model to training set: the prior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqYkW1lqXdZF"
      },
      "outputs": [],
      "source": [
        "# count the number of zeros and ones, AND twos, threes, etc.\n",
        "\n",
        "\n",
        "possible_numbers_vector = np.arange(10)\n",
        "count_vector= np.arange(10)\n",
        "\n",
        "for i in count_vector:\n",
        "    count_vector[i] = np.sum(ytrain == i)\n",
        "\n",
        "prob_of_numbers_vector = count_vector/ N_train #this vector contains the probability of each number to occur\n",
        "\n",
        "# Prints all the priors, rounded.  Dataset overtrains to 1's, undertrains 5's, but look for \n",
        "# the raw vector and see if there's anything under - repped maybe?\n",
        "\n",
        "# np.set_printoptions(threshold=np.inf) turn on for debugging\n",
        "print(prob_of_numbers_vector)\n",
        "\n",
        "for i in possible_numbers_vector:\n",
        "    print('Prior probability (Y = %d)  = %d/%d = %3.2f' % (possible_numbers_vector[i], count_vector[i], N_train, prob_of_numbers_vector[i]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra7okmUuXdZG"
      },
      "source": [
        "The code beneath is taken from Exercise 1 and it only handles digits 0 and 1. Change the code to handle all digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdyZ-Y8uXdZG"
      },
      "source": [
        "### Fitting the Naı̈ve-Bayes model to training set: the likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG_opmv2XdZG"
      },
      "outputs": [],
      "source": [
        "### split data base on its label value and put them in a matrix \n",
        "# fit model for zeros and ones separately\n",
        "\n",
        "p_numbers_vector = []\n",
        "for k in range(10):\n",
        "    # fit model for zeros and ones separately\n",
        "    Xtrain_num = Xtrain[ytrain == k, :]\n",
        "    # p(X_i = 1| Y = k) \n",
        "    p_numbers_vector.append(np.mean(Xtrain_num, axis=0))\n",
        "p_digits = p_numbers_vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0y3r0rIXdZG"
      },
      "outputs": [],
      "source": [
        "# This code should be sufficient, i.e. don't change this.\n",
        "def log_likelihood(x_new, p_digit):\n",
        "    pixel_log_lik = x_new*np.log(p_digit + 1e-16) + (1-x_new)*np.log(1-p_digit)\n",
        "    return np.sum(pixel_log_lik)\n",
        "    \n",
        "image_idxs_to_be_classified = [0,10,510,810]\n",
        "for image_idx in image_idxs_to_be_classified:\n",
        "    x_new = Xtrain[image_idx]\n",
        "    print('Image idx: %d (label=%d)' % (image_idx, ytrain[image_idx]))\n",
        "    \n",
        "    for i in range(10):\n",
        "        print('p(x_new | Y=%d): %3.2e' % (i ,np.exp(log_likelihood(x_new, p_digits[i]))))\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA6HoFwNXdZH"
      },
      "source": [
        "# Continue from this part, greetings Pat "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZIkom-lXdZH"
      },
      "source": [
        "### Implementing Bayes's theorem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEqa4zovXdZH"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvppO5wNvizu"
      },
      "outputs": [],
      "source": [
        "# Change the function to handle all digits (you may use for loop instead repeating the same code)\n",
        "\n",
        "# the issue is to do with classification\n",
        "# we have a valid array of all the posteriors generated but we are choosing the wrong one\n",
        "# there are different reasons for this but it is no coincidence we are picking the same\n",
        "# numbers as the overrepresented and underrepresented sets in the above\n",
        "\n",
        "# we need to find a way for the system to choose the highest posterior\n",
        "# I am assuming the issue is not with the actual classifier itself since that's\n",
        "# as simple as it gets\n",
        "\n",
        "def compute_posterior_prob(x_new):\n",
        "    # compute log likelihood\n",
        "    log_lik_zeros = log_likelihood(x_new, prob_of_numbers_vector[0])\n",
        "    log_lik_ones = log_likelihood(x_new, prob_of_numbers_vector[1])\n",
        "    log_lik_twos = log_likelihood(x_new, prob_of_numbers_vector[2])\n",
        "    log_lik_threes = log_likelihood(x_new, prob_of_numbers_vector[3])\n",
        "    log_lik_fours = log_likelihood(x_new, prob_of_numbers_vector[4])\n",
        "    log_lik_fives = log_likelihood(x_new, prob_of_numbers_vector[5])\n",
        "    log_lik_sixes = log_likelihood(x_new, prob_of_numbers_vector[6])\n",
        "    log_lik_sevens = log_likelihood(x_new, prob_of_numbers_vector[7])\n",
        "    log_lik_eights = log_likelihood(x_new, prob_of_numbers_vector[8])\n",
        "    log_lik_nines = log_likelihood(x_new, prob_of_numbers_vector[9])\n",
        "    # extend code here\n",
        "\n",
        "    # exponentiate\n",
        "    lik_zeros = np.exp(log_lik_zeros)\n",
        "    lik_ones = np.exp(log_lik_ones)\n",
        "    lik_twos = np.exp(log_lik_twos)\n",
        "    lik_threes = np.exp(log_lik_threes)\n",
        "    lik_fours = np.exp(log_lik_fours)\n",
        "    lik_fives = np.exp(log_lik_fives)\n",
        "    lik_sixes = np.exp(log_lik_sixes)\n",
        "    lik_sevens = np.exp(log_lik_sevens)\n",
        "    lik_eights = np.exp(log_lik_eights)\n",
        "    lik_nines = np.exp(log_lik_nines)\n",
        "    # extend code here\n",
        "\n",
        "    # implement eq. (4)\n",
        "    term_zeros = lik_zeros*prob_of_numbers_vector[0]\n",
        "    term_ones = lik_ones*prob_of_numbers_vector[1]\n",
        "    term_twos = lik_twos*prob_of_numbers_vector[2]\n",
        "    term_threes = lik_threes*prob_of_numbers_vector[3]\n",
        "    term_fours = lik_fours*prob_of_numbers_vector[4]\n",
        "    term_fives = lik_fives*prob_of_numbers_vector[5]\n",
        "    term_sixes = lik_sixes*prob_of_numbers_vector[6]\n",
        "    term_sevens = lik_sevens*prob_of_numbers_vector[7]\n",
        "    term_eights = lik_eights*prob_of_numbers_vector[8]\n",
        "    term_nines = lik_nines*prob_of_numbers_vector[9]\n",
        "    # extend code here\n",
        "    evidence = term_zeros+term_ones+term_twos+term_threes+term_fours+term_fives+term_sixes+term_sevens+term_eights+term_nines\n",
        "    \n",
        "    # insert your code here to return a vector of length 10 containing the posterior probability\n",
        "    # of belonging to each class:\n",
        "    post_prob_zero = term_zeros/evidence    # change this code\n",
        "    post_prob_one = term_ones/evidence     # change this code\n",
        "    post_prob_two = term_twos/evidence     # change this code\n",
        "    post_prob_three = term_threes/evidence   # change this code\n",
        "    post_prob_four = term_fours/evidence    # change this code\n",
        "    post_prob_five = term_fives/evidence    # change this code\n",
        "    post_prob_six = term_sixes/evidence     # change this code\n",
        "    post_prob_seven = term_sevens/evidence   # change this code\n",
        "    post_prob_eight = term_eights/evidence   # change this code\n",
        "    post_prob_nine = term_nines/evidence    # change this code\n",
        "\n",
        "    # Collect all probabilities into a vector\n",
        "    posterior = np.array([post_prob_zero,\\\n",
        "                          post_prob_one,\\\n",
        "                          post_prob_two,\\\n",
        "                          post_prob_three,\\\n",
        "                          post_prob_four,\\\n",
        "                          post_prob_five,\\\n",
        "                          post_prob_six,\\\n",
        "                          post_prob_seven,\\\n",
        "                          post_prob_eight,\\\n",
        "                          post_prob_nine]) \n",
        "    return posterior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg2UZgodXdZH"
      },
      "source": [
        "### A simple classification rule: take the class with largest posterior probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVZGslPrXdZI"
      },
      "outputs": [],
      "source": [
        "def classify(x):\n",
        "    posterior = compute_posterior_prob(x)\n",
        "    predicted_label = np.argmax(posterior)\n",
        "    return predicted_label\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwGIhLHXXdZI"
      },
      "source": [
        "### Change this code to classify some images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9FCCvHqXdZI"
      },
      "outputs": [],
      "source": [
        "# Classifies the first 10 images in the test set\n",
        "for i in range(100):\n",
        "    f = plt.figure()\n",
        "    \n",
        "    # this set of classification rules makes little sense to me\n",
        "    # but apparently they worked previously\n",
        "    # there should be no issue with Classify but\n",
        "    # we are still undoubtedly picking the wrong one\n",
        "    \n",
        "    # compute posterior probabilities\n",
        "    posterior = compute_posterior_prob(Xtest[i, :])\n",
        "    \n",
        "    # get true label and predicted label\n",
        "    true_label = ytest[i]\n",
        "    predicted_label = classify(Xtest[i, :])\n",
        "    \n",
        "    # show image \n",
        "    show_image(Xtest[i, :])\n",
        "    \n",
        "    # Print result\n",
        "    #print('p(Y|x) = (%2.1f)' % posterior)\n",
        "    print(posterior)\n",
        "    print(\"True label:\",true_label)\n",
        "    print(\"Predicted label:\",predicted_label)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx_5JhBwXdZI"
      },
      "source": [
        "### Let's compute the training and test errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_5KXxEPXdZI"
      },
      "outputs": [],
      "source": [
        "ytrain_hat = np.array([classify(x) for x in Xtrain])\n",
        "ytest_hat = np.array([classify(x) for x in Xtest])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwoxryLgXdZI"
      },
      "outputs": [],
      "source": [
        "mean_train_acc = np.mean(ytrain_hat == ytrain)\n",
        "mean_test_acc = np.mean(ytest_hat == ytest)\n",
        "print('Training accuracy:\\t%4.3f' % mean_train_acc)\n",
        "print('Test accuracy:\\t\\t%4.3f' % mean_test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2ISLIQxXdZJ"
      },
      "source": [
        "# 5. Compute the confusion matrix and explain what it shows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvO7eqx-XdZJ",
        "pycharm": {
          "name": "#%% Don't touch this code :)\n"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "labels = list(range(10))\n",
        "pred = np.array([classify(x) for x in Xtest])\n",
        "cm = confusion_matrix(y_true=ytest, y_pred=pred, labels=labels,normalize=\"true\")\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(cm)\n",
        "fig.colorbar(cax)\n",
        "ax.set_xticks(labels)\n",
        "ax.set_yticks(labels)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_yticklabels(labels)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiIZMX8qXdZJ"
      },
      "source": [
        "Write your explanation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sURAx3isXdZJ"
      },
      "source": [
        "# 6. Error analysis: find images that are misclassified by the system.  Are there common characteristics among the images that are misclassified?\n",
        "It's down to \"ornamentation\" from what I can determine after going over the array of posteriors but I can't prove it without fixing the system"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ass1_part1+2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
